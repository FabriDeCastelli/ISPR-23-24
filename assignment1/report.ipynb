{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Assignment 1: Time Series Analysis - Air Quality\n",
    "\n",
    "## Author: Fabrizio De Castelli\n",
    "\n",
    "### 1. Data Preprocessing\n",
    "Some basic data preprocessing to prepare the dataset for the analysis. Includes loading the dataset, filtering the columns, imputing missing values, and standardizing the dataset. The dataset is then visualized to better understand the upcoming analysis.\n",
    "\n",
    "#### 1.1 Load the dataset drop useless columns\n",
    "The dataset is loaded and the columns are filtered to keep only the sensors and the ground truths. The ground truths are the columns with the suffix 'GT'. The sensors are the columns with the suffix 'S' followed by a number.\n",
    "All columns have been converted to float and the missing values have been replaced with None. The NMHC(GT) column has been dropped as it contains too many missing values."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1c84fda0bc847429"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import Image\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T15:16:28.524769Z",
     "start_time": "2024-03-22T15:16:28.521871Z"
    }
   },
   "id": "9661259b51037a91",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "         Date      Time CO(GT)  PT08.S1(CO)  NMHC(GT) C6H6(GT)  PT08.S2(NMHC)  \\\n0  10/03/2004  18.00.00    2.6       1360.0     150.0     11.9         1046.0   \n1  10/03/2004  19.00.00      2       1292.0     112.0      9.4          955.0   \n2  10/03/2004  20.00.00    2.2       1402.0      88.0      9.0          939.0   \n3  10/03/2004  21.00.00    2.2       1376.0      80.0      9.2          948.0   \n4  10/03/2004  22.00.00    1.6       1272.0      51.0      6.5          836.0   \n\n   NOx(GT)  PT08.S3(NOx)  NO2(GT)  PT08.S4(NO2)  PT08.S5(O3)     T    RH  \\\n0    166.0        1056.0    113.0        1692.0       1268.0  13.6  48.9   \n1    103.0        1174.0     92.0        1559.0        972.0  13.3  47.7   \n2    131.0        1140.0    114.0        1555.0       1074.0  11.9  54.0   \n3    172.0        1092.0    122.0        1584.0       1203.0  11.0  60.0   \n4    131.0        1205.0    116.0        1490.0       1110.0  11.2  59.6   \n\n       AH  Unnamed: 15  Unnamed: 16  \n0  0.7578          NaN          NaN  \n1  0.7255          NaN          NaN  \n2  0.7502          NaN          NaN  \n3  0.7867          NaN          NaN  \n4  0.7888          NaN          NaN  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Date</th>\n      <th>Time</th>\n      <th>CO(GT)</th>\n      <th>PT08.S1(CO)</th>\n      <th>NMHC(GT)</th>\n      <th>C6H6(GT)</th>\n      <th>PT08.S2(NMHC)</th>\n      <th>NOx(GT)</th>\n      <th>PT08.S3(NOx)</th>\n      <th>NO2(GT)</th>\n      <th>PT08.S4(NO2)</th>\n      <th>PT08.S5(O3)</th>\n      <th>T</th>\n      <th>RH</th>\n      <th>AH</th>\n      <th>Unnamed: 15</th>\n      <th>Unnamed: 16</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10/03/2004</td>\n      <td>18.00.00</td>\n      <td>2.6</td>\n      <td>1360.0</td>\n      <td>150.0</td>\n      <td>11.9</td>\n      <td>1046.0</td>\n      <td>166.0</td>\n      <td>1056.0</td>\n      <td>113.0</td>\n      <td>1692.0</td>\n      <td>1268.0</td>\n      <td>13.6</td>\n      <td>48.9</td>\n      <td>0.7578</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>10/03/2004</td>\n      <td>19.00.00</td>\n      <td>2</td>\n      <td>1292.0</td>\n      <td>112.0</td>\n      <td>9.4</td>\n      <td>955.0</td>\n      <td>103.0</td>\n      <td>1174.0</td>\n      <td>92.0</td>\n      <td>1559.0</td>\n      <td>972.0</td>\n      <td>13.3</td>\n      <td>47.7</td>\n      <td>0.7255</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10/03/2004</td>\n      <td>20.00.00</td>\n      <td>2.2</td>\n      <td>1402.0</td>\n      <td>88.0</td>\n      <td>9.0</td>\n      <td>939.0</td>\n      <td>131.0</td>\n      <td>1140.0</td>\n      <td>114.0</td>\n      <td>1555.0</td>\n      <td>1074.0</td>\n      <td>11.9</td>\n      <td>54.0</td>\n      <td>0.7502</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10/03/2004</td>\n      <td>21.00.00</td>\n      <td>2.2</td>\n      <td>1376.0</td>\n      <td>80.0</td>\n      <td>9.2</td>\n      <td>948.0</td>\n      <td>172.0</td>\n      <td>1092.0</td>\n      <td>122.0</td>\n      <td>1584.0</td>\n      <td>1203.0</td>\n      <td>11.0</td>\n      <td>60.0</td>\n      <td>0.7867</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>10/03/2004</td>\n      <td>22.00.00</td>\n      <td>1.6</td>\n      <td>1272.0</td>\n      <td>51.0</td>\n      <td>6.5</td>\n      <td>836.0</td>\n      <td>131.0</td>\n      <td>1205.0</td>\n      <td>116.0</td>\n      <td>1490.0</td>\n      <td>1110.0</td>\n      <td>11.2</td>\n      <td>59.6</td>\n      <td>0.7888</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the dataset, replace commas with dots, and show the first 10 rows\n",
    "df = pd.read_csv('dataset/AirQualityUCI.csv', sep=';')\n",
    "df = df.replace(\",\", \".\", regex=True)\n",
    "df.head(5)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T15:16:28.567129Z",
     "start_time": "2024-03-22T15:16:28.526251Z"
    }
   },
   "id": "5cebc73a104c3427",
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "CO(GT)           1684\nC6H6(GT)          366\nNOx(GT)          1639\nNO2(GT)          1642\nPT08.S1(CO)       366\nPT08.S2(NMHC)     366\nPT08.S3(NOx)      366\nPT08.S4(NO2)      366\nPT08.S5(O3)       366\ndtype: int64"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ground truths\n",
    "ground_truths = df.filter(regex='(GT)')\n",
    "ground_truths = ground_truths.astype(float)\n",
    "ground_truths = ground_truths.replace(-200, None)\n",
    "\n",
    "# drop NMHC(GT) column as there are too many missing values\n",
    "ground_truths = ground_truths.drop(columns=['NMHC(GT)'])\n",
    "\n",
    "# sensors\n",
    "sensors = df.filter(regex='S\\d')\n",
    "sensors = sensors.replace(-200, None)\n",
    "\n",
    "# show columns with missing values and their number in a dataframe \n",
    "missing_values = ground_truths.isin([None]).sum()\n",
    "missing_values = pd.concat([missing_values, sensors.isin([None]).sum()])\n",
    "missing_values"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T15:16:28.578811Z",
     "start_time": "2024-03-22T15:16:28.567945Z"
    }
   },
   "id": "5bbf883c101eb73f",
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2 Imputation and standardization missing values\n",
    "Linear interpolation is used to impute missing values. This is done by taking the mean of the previous and next value. The dataset is then standardized by subtracting the column's mean from each column. The variance is not included in the standardization as it is not necessary for the upcoming analysis."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "79d23673b9192847"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def imputation(dataframe):\n",
    "    \"\"\"\n",
    "    Impute missing values with the mean of the previous and next value.\n",
    "    Done via linear interpolation.\n",
    "    \n",
    "    :param dataframe: the dataframe to impute\n",
    "    :return: the imputed dataframe\n",
    "    \"\"\"\n",
    "    for column in dataframe.columns[dataframe.isin([None]).any()]:\n",
    "        dataframe[column] = dataframe[column].astype(float).interpolate()\n",
    "    return dataframe\n",
    "\n",
    "def standardization(dataframe):\n",
    "    \"\"\"\n",
    "    Standardize the dataframe.\n",
    "    \n",
    "    :param dataframe: the dataframe to standardize\n",
    "    :return: the standardized dataframe\n",
    "    \"\"\"\n",
    "    for column in dataframe.columns:\n",
    "        dataframe[column] = dataframe[column].astype(float)\n",
    "        dataframe[column] -= dataframe[column].mean()\n",
    "    return dataframe\n",
    "\n",
    "sensors = imputation(sensors)\n",
    "sensors = standardization(sensors)\n",
    "ground_truths = imputation(ground_truths)\n",
    "ground_truths = standardization(ground_truths)\n",
    "    \n",
    "# assert that there are no missing values \n",
    "assert sensors.isin([None]).sum().sum() == 0\n",
    "assert ground_truths.isin([None]).sum().sum() == 0"
   ],
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-22T15:16:28.594536Z",
     "start_time": "2024-03-22T15:16:28.580255Z"
    }
   },
   "id": "initial_id",
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.4 Statistics: starting and ending day and time of measurements\n",
    "To better understand the upcoming analysis, it is important to know the starting and ending day and time of the measurements. The first measurement has been done in the rush hour (traffic peak): the sensors can capture pollution the most."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "745f17d771e9f3bf"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Measurements starting day: Sunday, 10/03/2004, time: 18.00.00\n",
      "Measurements ending day: Monday, 04/04/2005, time: 14.00.00\n"
     ]
    }
   ],
   "source": [
    "first_day = df[\"Date\"].iloc[df[\"Date\"].first_valid_index()]\n",
    "print(f'Measurements starting day: {pd.to_datetime(first_day).day_name()}, {first_day}, '\n",
    "      f'time: {df[\"Time\"].iloc[df[\"Time\"].first_valid_index()]}')\n",
    "\n",
    "last_day = df[\"Date\"].iloc[df[\"Date\"].last_valid_index()]\n",
    "print(f'Measurements ending day: {pd.to_datetime(last_day).day_name()}, {last_day}, '\n",
    "        f'time: {df[\"Time\"].iloc[df[\"Time\"].last_valid_index()]}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-22T15:16:28.600220Z",
     "start_time": "2024-03-22T15:16:28.595579Z"
    }
   },
   "id": "a998f5852f9cc5de",
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sensors.plot(subplots=True, figsize=(20, 20))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-03-22T15:16:28.600864Z"
    }
   },
   "id": "cd64c8e76eefa61d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Some utility functions"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f4462a9be840a914"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def normalized_cross_correlation(x, y, lag):\n",
    "    \"\"\"\n",
    "    Computes the normalized cross-correlation between two time series, for a given lag.\n",
    "    \n",
    "    :param x: the first time series\n",
    "    :param y: the second time series\n",
    "    :param lag: the lag\n",
    "    :return: the normalized cross-correlation\n",
    "    \"\"\"\n",
    "    return cross_correlation(x, y, lag) / (np.linalg.norm(x) * np.linalg.norm(y))\n",
    "\n",
    "def cross_correlation(x, y, lag):\n",
    "    \"\"\"\n",
    "    Computes the cross-correlation between two time series, for a given lag.\n",
    "    \n",
    "    :param x: the first time series\n",
    "    :param y: the second time series\n",
    "    :param lag: the lag\n",
    "    :return: the cross-correlation\n",
    "    \"\"\"\n",
    "    range_ = slice(max(0, lag), min(len(x), len(x) + lag))\n",
    "    return np.dot(np.roll(x, -lag)[range_], y[range_])\n",
    "    \n",
    "def plot_cross_correlation(correlations, sensor1, sensor2, lags, show=True, save=True):\n",
    "    \"\"\"\n",
    "    Plots the cross-correlation between all the columns of the dataframe.\n",
    "    \n",
    "    :param correlations: the normalized cross-correlations\n",
    "    :param sensor1: the first sensor's name \n",
    "    :param sensor2: the second sensor's name\n",
    "    :param lags: the lags at which the cross-correlation was computed\n",
    "    :param show: whether to show the plot\n",
    "    :param save: whether to save the plot\n",
    "    \"\"\"\n",
    "    assert save or show\n",
    "    \n",
    "    # plot with seaborn\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.set_context('paper')\n",
    "    sns.set_style('whitegrid')\n",
    "    sns.lineplot(x=lags, y=correlations)\n",
    "    plt.title(f'{sensor1} vs {sensor2}')\n",
    "    plt.xlabel('Lag')\n",
    "    plt.ylabel('Cross - Correlation')\n",
    "    plt.axhline(0, color='red', lw=1, linestyle='--')\n",
    "    if save:\n",
    "        if \"GT\" in sensor1:\n",
    "            path = 'results/ground_truths'\n",
    "        else:\n",
    "            path = 'results/sensors'\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "        plt.savefig(f'{path}/{sensor1}_vs_{sensor2}.png')\n",
    "    if show:\n",
    "        plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "def save_all_pairs(dataframe, lags):\n",
    "    \n",
    "    for i, column1 in enumerate(dataframe.columns):\n",
    "        for j, column2 in enumerate(dataframe.columns):\n",
    "            correlations = [\n",
    "                normalized_cross_correlation(dataframe[column1], dataframe[column2], lag) \n",
    "                for lag in lags\n",
    "            ]\n",
    "            plot_cross_correlation(correlations, column1, column2, lags, show=False)\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9789550b8939d7ab",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Cross-Correlation\n",
    "Cross-correlation: measure of similarity between two time series as a function of the lag of one relative to the other. It is defined as:\n",
    "\n",
    "$$\n",
    "\\phi_{x_1x_2}(\\tau) = \\sum_{t=\\max(0, \\tau)}^{N + \\min(0, \\tau) - 1} x_1(t - \\tau)x_2(t)\n",
    "$$\n",
    "\n",
    "where $\\tau$ is the lag, $N$ is the length of the time two time series, $x_1_t$ and $x_2_t$ are the time series, and $\\bar{x_1}$ and $\\bar{x_2}$ are the means of the time series. The normalized cross-correlation, instead, is defined as:\n",
    "\n",
    "$$\n",
    "\\bar{\\phi}_{x_1x_2}(\\tau) = \\frac{\\phi_{x_1x_2}(\\tau)}{\\sqrt{{\\displaystyle \\sum_{t=1}^{N}} (x_1(t))^2} \\sqrt{{\\displaystyle \\sum_{t=1}^{N}} (x_2(t))^2}}\n",
    "$$\n",
    "\n",
    "It can be helpful for the analysis to plot the auto-correlation of a time series, which is its cross-correlation with itself.\n",
    "\n",
    "$$\n",
    "\\rho_{x_1}(\\tau) = \\phi_{x_1x_1}(\\tau) \n",
    "$$"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a988e498976088f2"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "WEEK_HOURS = 7 * 24\n",
    "num_weeks = 3\n",
    "taus = range(-num_weeks * WEEK_HOURS, num_weeks * WEEK_HOURS + 1)\n",
    "            \n",
    "# saves the cross-correlation between all pairs of sensors\n",
    "save_all_pairs(sensors, taus)\n",
    "# same for ground truths\n",
    "save_all_pairs(ground_truths, taus)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e7ba46f690bef3a2",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Results\n",
    "In this analysis we ony propose the (normalized) cross-correlation between two pairs of sensors. It is computed for a range of lags from -504 to 504, the equivalent of the hours of two times 3 weeks.\n",
    "\n",
    "#### 3.1 Cross-Correlation between S1(CO) and S3(NOx)\n",
    "THe first pair is S1(CO), which measures Carbon Monoxide, and S3(NOx), which measures Nitrogen Oxides:\n",
    "1. The cross-correlation (in module) is highest at a lag of 0, which can be expected since the sensors measure something that can be very correlated at the same time. \n",
    "2. Each local peak in the cross-correlation happens at an interval of 24 hours, meaning that every day the two sensors are measuring some elements that might be correlated. Since the time series starts at 18:00, we have that each peak happens at the same time for the following days and this can be interpreted as the \"rush hour\" at which most of the cars are on the road. \n",
    "3. This analysis shows that these two sensors are measuring chemicals that are inversely correlated, as confirmed by  [this paper](https://www.sciencedirect.com/science/article/pii/S0196890404000494): *\"an increase of NOx emission is accompanied by a decrease of CO emission, the two phenomena being antagonistic\"*. Of course, this is a general statement and the cross-correlation is not a causation, but it is interesting to see that the data supports this theory, even if the reference paper does not involve measurements of air pollution caused by traffic.\n",
    "4. The strength of the correlation of these sensors is not very high, but suggests the presence of a relationship between the two chemicals. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "83043324f63a9428"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "display(Image(filename='results/sensors/PT08.S1(CO)_vs_PT08.S3(NOx).png'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ab3880c0c11e91",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.2 Cross-Correlation between S4(NO2) and S2(NMHC)\n",
    "The second pair is S4(NO2), which measures Nitrogen Dioxide, and S2(NMHC), which measures Non-Methane Hydrocarbons:\n",
    "1. As for the previous comparison, the cross-correlation is highest at a lag of 0.\n",
    "2. The cross-correlation is a little bit stronger than the previous one, but still not very high. We can infer it is higher from the plot: the peaks are higher in module than the previous ones.\n",
    "3. The chemicals measured by these sensors are not inversely correlated as the previous ones."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "896b12ee4053d3b7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "display(Image(filename='results/sensors/PT08.S4(NO2)_vs_PT08.S2(NMHC).png'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb6834134058d5c0",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.3 Cross-Correlation between S4(NO2) and S5(O3)\n",
    "In this case we have a weaker cross-correlation between the two sensors (with respect to the previous experiments). The cross-correlation is close to 0 for all lags, even if we can see the same trend as always in which the strength increases for lags that are multiples of 24.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7760e737a6a841d1"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "display(Image(filename='results/sensors/PT08.S4(NO2)_vs_PT08.S5(O3).png'))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6ea68838ca7bf90d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.4 Auto-Correlation of S4(NO2)\n",
    "It is interesting to plot the Auto-Correlation of a signal. Among all signals we chose to plot the Auto-Correlation of S4(NO2), which measures Nitrogen Dioxide, because it has higher auto-correlation than the other sensors. Even if for the analysis it was not required to plot the auto-correlation, here are some considerations:\n",
    "1. The auto-correlation is 1 at a lag of 0, as expected.\n",
    "2. The trend is still the same as the previous plots, with peaks at multiples of 24.\n",
    "3. Since NO2 is produced mostly by the combustion process of diesel engines (mostly trucks, buses and so on..., [source](https://www.lung.org/clean-air/outdoors/what-makes-air-unhealthy/nitrogen-dioxide)), a possible interpretation of this plot is that in proximity of the sensor many vehicles implementing this type of engine are passing. We are not sure about this but the consistency in measurements of this sensor suggests that more or less a regular amount of NO2 is produced every day."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "84c3a4961294845e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_cross_correlation(\n",
    "    [normalized_cross_correlation(sensors['PT08.S4(NO2)'], sensors['PT08.S4(NO2)'], lag) \n",
    "     for lag in taus],\n",
    "    'PT08.S4(NO2)', \n",
    "    'PT08.S4(NO2)', \n",
    "    taus, \n",
    "    save=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "235b91776a804f8e",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.5 Cross-Correlation between S1(CO) and CO(GT)\n",
    "The last comparison is between the sensor S1(CO) and its ground truth CO(GT). What is curious about this plot is that we expect the sensor to be correlated positively with the ground truth, but the cross-correlation is negative for the majority of lags. Moreover, since for $\\tau = 0$ the Cross-Correlation is the correlation of the perfectly aligned signals, that it is slightly bigger than 0.1, it is an index of poor correlation. Here are some possible motivations:\n",
    "1. The sensor is not very accurate or the ground truth is not very reliable.\n",
    "2. The sensor is measuring something that is not exactly the same as the ground truth, maybe sensitive to other pollutants.\n",
    "3. The location of the sensor that generated the ground truth has been placed in a place which was not really affected the same way by the pollution that the sensor was measuring. In the dataset description: *\"Ground Truths ... were provided by a co-located reference certified analyzer\"*, so we don't know if the sensor was placed in the same place as the ground truth. "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f89457d50b18e73"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "plot_cross_correlation(\n",
    "    [normalized_cross_correlation(sensors['PT08.S4(NO2)'], ground_truths['NO2(GT)'], lag) \n",
    "     for lag in taus],\n",
    "    'PT08.S4(NO2)', \n",
    "    'NO2(GT)', \n",
    "    taus, \n",
    "    save=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f0f5842c095e764d",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Conclusions\n",
    "We know that the dataset is built on top of measurements at road level of air pollution in an italian city. More or less all sensors are correlated, but not all of them are strongly. We examined the correlation between S1 and S3, which are inversely correlated, but their correlation is not very strong as overall. The same goes for S4 and S2, which are not inversely correlated and exhibit a stronger (not that much) correlation. Then, we plotted a case in which what we measured appeared to be weaker than the previous cases, which is the cross-correlation between S4 and S5, but still being consistent with the trend we are expecting: the peaks are at multiples of 24. As second to last experiment, we plotted the auto-correlation of S4, which is the sensor that measures Nitrogen Dioxide. The auto-correlation is consistent with the previous plots and suggests that the sensor is measuring a regular amount of NO2 every day, of course with peaks during the rush hour.\n",
    "\n",
    "The analysis has been done for lags of 3 weeks, which is a reasonable amount of time to capture the behaviour of the sensors. Lags could have been chosen for all the time series we decided to focus on a smaller time span to shift the focus on meaningful patterns in the signals, assuming that for larger taus the analysis is very similar to the one we did.\n",
    "\n",
    "To conclude: even if the examination and hence interpretation of these Cross-Correlation is influenced by the noise in the data, we still managed to get interpretable results. To analyze and goo deeper in the analysis, we should study the behaviour of these chemical considering other environmental factors, such as temperature, humidity and weather (for example wind speed). But with the used tools we witnessed that the sensors are measuring something which is somehow correlated as overall and that the measurements are aligned with what we can say to be intuitive about air quality in a (possibly big) city, measured at road level."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "26b262e8ca66f504"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "2f66d17415a95bc5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
